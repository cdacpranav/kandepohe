{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Load the pretrained VGG16 model without top layers\n",
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze most layers\n",
    "for layer in base_model.layers[:-5]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Custom classification head\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(256, activation=\"relu\", kernel_regularizer=l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(8, activation=\"softmax\")(x)  # 8 emotion classes\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries for file handling and numerical operations\n",
    "import os      \n",
    "import random    \n",
    "import numpy as np \n",
    "import pandas as pd  \n",
    "\n",
    "# To Displays progress bars for loops\n",
    "from tqdm import tqdm \n",
    "\n",
    "# Audio processing libraries\n",
    "import librosa\n",
    "import librosa.display  \n",
    "\n",
    "# Visualization libraries\n",
    "import seaborn as sns  \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "# To evaluate model performance using a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix  \n",
    "\n",
    "# Preprocessing utilities\n",
    "from sklearn.model_selection import train_test_split \n",
    "# Converts categorical labels to numerical labels \n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "# Normalizes features to improve model performance\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "# Deep learning model training using TensorFlow Keras\n",
    "from tensorflow.keras.models import Sequential  \n",
    "from tensorflow.keras.layers import Dense, Dropout  \n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten  \n",
    "\n",
    "# Model evaluation utilities\n",
    "from tensorflow.keras import models as tf_models  \n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataset Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = r\"I:\\My Drive\"\n",
    "dataset_subfolders = [\"Audio_Speech_Actors_01-24\", \"Audio_Song_Actors_01-24\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define emotions based on RAVDESS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_map = {'01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
    "               '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensuring dataset_features directory exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r\"I:\\My Drive\\SpeechEmotionDetection\\dataset_features\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "### Audio Feature Extraction\n",
    "This function extracts three key features from an audio file:\n",
    "1. **MFCCs (Mel-Frequency Cepstral Coefficients)**\n",
    "    - Captures timbral characteristics of speech.\n",
    "    - Timbre refers to the quality or color of a sound that makes it distinct from other sounds, even when they have the same pitch and loudness.\n",
    "2. **Chroma Features** \n",
    "    - Represents pitch class distribution in the audio.\n",
    "3. **Mel Spectrogram** \n",
    "    - Shows energy distribution over different frequencies.\n",
    "\n",
    "These features are essential for training a machine learning model to classify speech emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path):\n",
    "    \"\"\"\n",
    "    Extracts audio features (MFCCs, Chroma, and Mel Spectrogram) from a given audio file.\n",
    "    And returns a feature vector (numpy array) containing MFCCs, Chroma, and Mel Spectrogram.\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "    # Extract MFCCs\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n",
    "\n",
    "    # Extract Chroma\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr).T, axis=0)\n",
    "\n",
    "    # Extract Mel Spectrogram\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=y, sr=sr).T, axis=0)\n",
    "\n",
    "    return np.hstack([mfccs, chroma, mel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a dataset folder\n",
    "def process_dataset(dataset_path, category, data):\n",
    "    \"\"\"Processes audio files from a dataset directory, extracts features and stores them in strucuted list\"\"\"\n",
    "    # List to store all file paths and names\n",
    "    all_files = []\n",
    "    # Iterate through each actor's folder inside the dataset directory\n",
    "    for actor_folder in os.listdir(dataset_path):\n",
    "        actor_path = os.path.join(dataset_path, actor_folder)\n",
    "        if not os.path.isdir(actor_path):\n",
    "            continue\n",
    "\n",
    "        # Iterate through audio files inside each actor's folder\n",
    "        for file_name in os.listdir(actor_path):\n",
    "            file_path = os.path.join(actor_path, file_name)\n",
    "\n",
    "            # Ignore non-audio files (only process .wav files)\n",
    "            if not file_name.lower().endswith(\".wav\"):\n",
    "                continue  # Skip non-audio files\n",
    "\n",
    "            all_files.append((file_path, file_name))\n",
    "\n",
    "\n",
    "    # Display processing of files found in the category\n",
    "    print(f\"Processing {category}: {len(all_files)} files\")\n",
    "\n",
    "    # Process each audio file and extract features\n",
    "    for file_path, file_name in tqdm(all_files, desc=f\"{category}\"):\n",
    "        try:\n",
    "            features = extract_features(file_path)\n",
    "            emotion_code = file_name.split(\"-\")[2]\n",
    "            emotion = emotion_map.get(emotion_code, \"unknown\")\n",
    "            data.append([file_path, category, emotion] + features.tolist())\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing empty list to store extracted features\n",
    "data = []\n",
    "\n",
    "# Iterate through each subfolder in the dataset\n",
    "for subfolder in dataset_subfolders:\n",
    "    process_dataset(os.path.join(dataset_root, subfolder), subfolder, data)\n",
    "\n",
    "# Check if feature extraction was successful\n",
    "if data:\n",
    "    # Define Dataframe to store metadata and extracted features\n",
    "    columns = [\"file_path\", \"category\", \"emotion\"] + [f\"feature_{i}\" for i in range(len(data[0]) - 3)]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    #Converting Dataframe to csv\n",
    "    df.to_csv(os.path.join(output_dir, \"audio_features.csv\"), index=False)\n",
    "    print(\"Feature extraction completed! Data saved to dataset_features/audio_features.csv\")\n",
    "else:\n",
    "    print(\"No valid audio files found for feature extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved CSV file for verification\n",
    "df = pd.read_csv(r\"I:\\My Drive\\SpeechEmotionDetection\\dataset_features\\audio_features.csv\")\n",
    "\n",
    "# Display Top 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Sample Waveform and MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select audio file from dataset\n",
    "audio_file = random.choice(df['file_path'])\n",
    "\n",
    "# Load audio file\n",
    "y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "# Compute MFCCs\n",
    "mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, ax = plt.subplots(nrows=2, figsize=(10, 6))\n",
    "\n",
    "# Plot waveform\n",
    "ax[0].set_title(f\"Waveform of {audio_file.split('/')[-1]}\")\n",
    "librosa.display.waveshow(y, sr=sr, ax=ax[0])\n",
    "ax[0].set_xlabel(\"Time\")\n",
    "ax[0].set_ylabel(\"Amplitude\")\n",
    "\n",
    "# Plot MFCC features\n",
    "img = librosa.display.specshow(mfccs, x_axis=\"time\", sr=sr, ax=ax[1], cmap=\"viridis\")\n",
    "ax[1].set_title(\"MFCC Features\")\n",
    "ax[1].set_xlabel(\"Time\")\n",
    "ax[1].set_ylabel(\"MFCC Coefficients\")\n",
    "fig.colorbar(img, ax=ax[1], format=\"%+2.f\")\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding emotions\n",
    "print(df['emotion'])\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"emotion\"] = label_encoder.fit_transform(df[\"emotion\"])\n",
    "df['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining columns to exclude non-feature columns\n",
    "exclude_columns = [\"emotion\", \"file_path\", \"category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and labels\n",
    "X = df.drop(columns=exclude_columns, axis=1)\n",
    "y = df[\"emotion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=28)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is **.npy**?\n",
    "* NumPy Binary Format is a file format used by NumPy to store arrays efficiently. \n",
    "* *It saves arrays in binary format that retains their structure and datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving preprocessed feature arrays and labels as .npy files for quick loading\n",
    "\n",
    "np.save(os.path.join(output_dir, \"X_train.npy\"), X_train)\n",
    "\n",
    "np.save(os.path.join(output_dir, \"X_test.npy\"), X_test)\n",
    "\n",
    "np.save(os.path.join(output_dir, \"y_train.npy\"), y_train)\n",
    "\n",
    "np.save(os.path.join(output_dir, \"y_test.npy\"), y_test)\n",
    "\n",
    "print(\"Data preprocessing completed! Datasets saved in dataset_features/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_dir = r\"I:\\My Drive\\SpeechEmotionDetection\\dataset_features\"\n",
    "X_train = np.load(os.path.join(dataset_dir, \"X_train.npy\"))\n",
    "X_test = np.load(os.path.join(dataset_dir, \"X_test.npy\"))\n",
    "y_train = np.load(os.path.join(dataset_dir, \"y_train.npy\"))\n",
    "y_test = np.load(os.path.join(dataset_dir, \"y_test.npy\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining CNN Model\n",
    "- This model uses **1D convolutional layers** to extract meaningful features from **audio input sequences**.  \n",
    "- Using **Sequential layer** as it is a linear stack of layers in Keras, used to build neural network models where each layer has exactly one input tensor and one output tensor. It is simple to use but only supports layer-by-layer stacking.\n",
    "- **Pooling layers** reduce dimensions and prevent overfitting.  \n",
    "- The **Dense layer and Dropout** enhance learning, followed by **Softmax** for final classification.  \n",
    "- The final layer has neurons equal to the **number of unique emotions** in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Load the pretrained VGG16 model without top layers\n",
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze most layers\n",
    "for layer in base_model.layers[:-5]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Custom classification head\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(256, activation=\"relu\", kernel_regularizer=l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(8, activation=\"softmax\")(x)  # 8 emotion classes\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the Model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Callbacks to prevent overfitting\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[early_stop, reduce_lr])\n",
    "\n",
    "# Plot Training vs Validation Loss\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(history.history['loss'], label=\"Training Loss\", color='blue')\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\", color='orange')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "model.save(r\"I:\\My Drive\\SpeechEmotionDetection\\models\\speech_emotion_model.h5\")\n",
    "print(\"Model trained and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation 📊\n",
    "- Model evaluation is the process of assessing how well a trained model performs on unseen data. It helps determine accuracy, robustness, and generalization before deployment.\n",
    "- Once the model is trained, we evaluate its performance using various metrics:\n",
    "    1. **Accuracy** - Measures overall correctness.\n",
    "    2. **Precision & Recall** - Important for imbalanced datasets.\n",
    "    3. **F1-Score** - Balances precision and recall.\n",
    "    4. **Confusion Matrix** - Provides a detailed view of model errors.\n",
    "    5. **Loss Function** - Determines model optimization.\n",
    "\n",
    "We will use **Scikit-learn** to compute these metrics and visualize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input\n",
    "X_test = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Saved model\n",
    "model = tf_models.load_model(\"I:\\My Drive\\SpeechEmotionDetection\\models\\speech_emotion_model_vgg16.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Output\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Print\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Visualization 🎯\n",
    "A **confusion matrix** is a useful tool to evaluate the performance of a classification model. It shows:\n",
    "- **Correct classifications** along the diagonal.\n",
    "- **Misclassifications** off the diagonal.\n",
    "\n",
    "We use **Seaborn’s heatmap** to visually analyze model errors and performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
